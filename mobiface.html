<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Topic of group 47: Mobile Face Tracking</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <link href="css/styles.css" rel="stylesheet">

  </head>

  <body>
    <!-- Navigation bar-->
    <nav class="navbar navbar-expand-lg bg1 fixed-top text-uppercase" id="mainNav">
      <div class="container">
        <a class="Home" href="index.html">
            [HOME]
        </a>
        <div>
          <ul class="navbar-nav">
            <li class="nav-item">
              <a class="nav-link" href="profile.html">About Yiming and his team</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="mobiface.html">Their project: Mobiface</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

<header class="bg-mobiFace">
</header>

<header class="Title-subpages">
      <div class="container">
      <h1 class="text-uppercase mb-0">MobiFace</h1>
          <h5><a href="https://mobiface.github.io/">Link to Mobiface.io</a></h5>
          <br>
      <h4 class="content">
    <p>Yiming and his team have gathered material and created a publicly available database of smartphone recorded selfie-style videos, intended to serve as the backbone of machine learning based face tracking algorithms.  
    <p>The database includes a whopping 95,635 frames, with a bounding box of the face “manually labelled” on each frame. Out of hundreds of available mobile recordings 80 videos were carefully selected based on various criteria such as the ratio of the time in which the face remains in the frame, the speed of camera movement or the extremity of lighting conditions. This ensures that the selected clips provide valuable and realistic content.
    </p> 
    <p>Regular object tracking is cannot be sufficiently applied for software implementations that require face tracking on mobile devices due to the unpredictability of certain factors, 4 of which are highlighted in Yiming’s paper.
    </p>
    <p>First, face sizes in self recorded mobile videos can change drastically as both the device and its target are in constant movement, opposed to remaining still as for example desktop web cameras do. This continuous and quick change of position and location also raises the possibility of the target leaving and reentering the frame multiple times. The face tracking engine must recognise this, as well as keep up with fast camera movement. Additionally, it is not uncommon for more than one face to be present in a single frame, and it is essential that the algorithm differentiates and keeps track of each individually.
    </p>
    <p>It is important for researchers to receive feedback and recognition from the international academic and scientific community, which helps them with future progress. MobiFace has been accepted by 14th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2019). This annual/biannual conference takes place in France in 2019, and researchers get a chance to broadcast their findings to the community via demos, presentations and exhibitions.
    </p>
      </h4>
      </div>
      </header>
    


    <!-- Footer -->
    <footer class="footer text-center">
      <div class="container">
        <div class="row">
          <div class="col-md-4 mb-5 mb-lg-0">
            <h4 class="text-uppercase mb-4">Location</h4>
            <p class="lead mb-0">Imperial College London
              <br>South Kensington Campus
London SW7 2AZ</p>
          </div>
          </div>
        </div>
    </footer>

    <div class="copyright text-center text-white">
      <div class="container">
        <small>Copyright &copy;2019 Group 47</small>
      </div>
    </div>

  </body>

</html>
